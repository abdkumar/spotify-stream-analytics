{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b22692b8-cad4-46b5-aa27-42f2990b2e83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[SecretScope(name='spotify-scope'), SecretScope(name='spotify-secret-scope')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdcf4fec-8f8e-4e06-9555-7ad2fbecaf72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from delta.tables import DeltaTable\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1796c2-f591-4cb8-bf13-ba5a0f3f48d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set Snowflake credentials as environment variables\n",
    "snowflake_args = {'sfURL': dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"sfURL\"),\n",
    " 'sfUser': dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"sfUser\"),\n",
    " 'sfPassword': dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"sfPassword\"),\n",
    " 'sfDatabase': dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"sfDatabase\"),\n",
    " 'sfSchema': dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"sfSchema\"),\n",
    " 'sfWarehouse': dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"sfWarehouse\"),\n",
    " 'sfRole': dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"sfRole\"),\n",
    " 'dbtable': dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"dbtable\")\n",
    " }\n",
    "\n",
    "snowflake_args['preactions'] = 'USE DATABASE '+ snowflake_args['sfDatabase'] + \";USE SCHEMA \" + snowflake_args['sfSchema'] + \";\"\n",
    " \n",
    "ADLS_STORAGE_ACCOUNT_NAME = dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"adls-account-name\")\n",
    "ADLS_ACCOUNT_KEY = dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"adls-account-key\")\n",
    "ADLS_CONTAINER_NAME = dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"adls-container-name\")\n",
    "ADLS_FOLDER_PATH = dbutils.secrets.get(scope=\"spotify-secret-scope\", key=\"adls-folder-path\")\n",
    "\n",
    "DELTA_SOURCE = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"+ ADLS_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa92d2d-bd9e-405a-87ee-ec9b3ccca09c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define checkpoint file path\n",
    "CHECKPOINT_FILE_PATH = \"./delta_checkpoint.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258b9d6d-c1cc-4778-b572-750daf101ac4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    ADLS_ACCOUNT_KEY,\n",
    ")\n",
    "spark.conf.set(\"spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c8d5a6f-dea2-4bec-aae0-f86c8f7ed7bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the checkpoint file if it exists, otherwise set the LAST_READ_TIMESTAMP to 0\n",
    "delta_table = DeltaTable.forPath(spark, DELTA_SOURCE)\n",
    "try:\n",
    "    with open(CHECKPOINT_FILE_PATH, \"r\") as checkpoint_file:\n",
    "        LAST_READ_TIMESTAMP = checkpoint_file.read()\n",
    "    print(\"Getting Latest timtestamp from checkpoint file: \")\n",
    "except FileNotFoundError:\n",
    "    print(\"Getting Latest timtestamp from delta history: \")\n",
    "    LAST_READ_TIMESTAMP = (\n",
    "        delta_table.history().select(F.min(\"timestamp\").alias(\"timestamp\")).collect()\n",
    "    )\n",
    "    LAST_READ_TIMESTAMP = str(LAST_READ_TIMESTAMP[0][\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93cf17db-5a8b-4bdd-be8d-52a31b36ce80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST_READ_TIMESTAMP:  2023-12-13 20:50:44\n"
     ]
    }
   ],
   "source": [
    "print(\"LAST_READ_TIMESTAMP: \", LAST_READ_TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a47f9f-fb5c-4312-83bd-8f56feeb1171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_delta_to_snowflake():\n",
    "    delta_df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .option(\"startingTimestamp\", LAST_READ_TIMESTAMP)\n",
    "        .load(DELTA_SOURCE)\n",
    "        )\n",
    "    \n",
    "    if delta_df.count()!=0:\n",
    "        excluded_columns = [\"_change_type\", \"_commit_version\", \"_commit_timestamp\"]\n",
    "        selected_columns = [\n",
    "            column for column in delta_df.columns if column not in excluded_columns\n",
    "            ]\n",
    "        # Write DataFrame to Snowflake table\n",
    "        delta_df.select(selected_columns).write.format(\"net.snowflake.spark.snowflake\").options(**snowflake_args).mode(\"append\").save()\n",
    "\n",
    "        updated_timestamp = delta_table.history().select(F.max(\"timestamp\").alias(\"timestamp\")).collect()[0][\"timestamp\"]\n",
    "        updated_timestamp += datetime.timedelta(seconds=1)\n",
    "        updated_timestamp = str(updated_timestamp)\n",
    "        print(\"Updated timestamp: \", updated_timestamp)\n",
    "        with open(CHECKPOINT_FILE_PATH, \"w\") as file:\n",
    "            file.write(updated_timestamp)\n",
    "\n",
    "    else:\n",
    "        print(\"Latest delta files already ingested to snowflake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d10dea31-b696-452e-a15f-0265f8b7c3c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La[REDACTED] delta files already ingested to snowflake\n"
     ]
    }
   ],
   "source": [
    "save_delta_to_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a916911-043b-4631-b7fb-bde776785258",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 235607717215574,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta_to_Snowflake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
